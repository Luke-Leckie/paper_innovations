{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03715d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH=\"/home/ll16598/Documents/INNOVATE\"\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "#import ruptures as rpt\n",
    "import warnings\n",
    "#import ruptures\n",
    "#import requests\n",
    "import time\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee8567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./Data/Derivative_Datasets/jump_data.csv')\n",
    "paper_titles = list(set(df['paper_title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac757e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/papers-with-abstracts.json') as f:\n",
    "    pwc=json.load(f)\n",
    "title_to_abstract = {item['title']: item['abstract'] for item in pwc}\n",
    "len(title_to_abstract.values())\n",
    "pwc=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941ecf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/paper-code-links.json') as f:\n",
    "    paper_code=json.load(f)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b33def7-4522-4f6e-9a43-3f9eb99dc3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256224"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6142442-54f3-40c4-a8cb-22946158617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from github import Github\n",
    "import pprint\n",
    "import gzip\n",
    "\n",
    "# Optionally, set your GitHub access token for higher rate limits:\n",
    "access_token = \"\"  # Replace with your token or None\n",
    "g = Github(access_token) if access_token else Github()\n",
    "\n",
    "# Sample paper_code list for demonstration (your actual list may be larger)\n",
    "\n",
    "def get_repo_from_url(repo_url):\n",
    "    \"\"\"\n",
    "    Extracts the owner and repository name from a GitHub repo URL.\n",
    "    Example: 'https://github.com/user/repo' -> ('user', 'repo')\n",
    "    \"\"\"\n",
    "    pattern = r\"github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.search(pattern, repo_url)\n",
    "    if match:\n",
    "        owner, repo_name = match.groups()\n",
    "        # Remove any trailing characters such as '.git'\n",
    "        repo_name = repo_name.replace('.git', '')\n",
    "        return owner, repo_name\n",
    "    return None, None\n",
    "\n",
    "def get_all_python_files(repo, extensions=(\".py\", \".ipynb\")):\n",
    "    \"\"\"\n",
    "    Recursively retrieves all files with the specified extensions in the given repository.\n",
    "    Returns a list of dictionaries with file information.\n",
    "\n",
    "    Parameters:\n",
    "        repo: The PyGithub Repository object.\n",
    "        extensions (tuple): A tuple of file extensions to include.\n",
    "                            Defaults to (\".py\", \".ipynb\").\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries containing file information.\n",
    "                    Each dictionary has keys: \"path\", \"download_url\", and \"name\".\n",
    "    \"\"\"\n",
    "    files_found = []\n",
    "    contents = repo.get_contents(\"\")\n",
    "    \n",
    "    while contents:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        else:\n",
    "            if file_content.name.endswith(extensions):\n",
    "                files_found.append({\n",
    "                    \"path\": file_content.path,\n",
    "                    \"download_url\": file_content.download_url,\n",
    "                    \"name\": file_content.name\n",
    "                })\n",
    "    return files_found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61fa78a2-ae8f-45f8-9a4d-067a5217b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    # Replace any character that is not alphanumeric, underscore, dash, or space with an underscore.\n",
    "    return re.sub(r'[^\\w\\-_ ]', '_', filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39697bd5-ceab-4ecf-8fda-b62dec5aa831",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_titles=os.listdir('/media/ll16598/One Touch/INNOVATE/pwc_python_files_from_git/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdaf4d46-c50c-4b57-bd8c-6da3c446fac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22925"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(completed_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77a927-144e-42e9-a339-9790383390c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing repo https://github.com/h3nok/OCM: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948\n",
      "Could not parse repo_url: https://worksheets.codalab.org/worksheets/0xecc9a01cfcbc4cd6b0444a92d259a87c\n",
      "Error processing repo https://github.com/KishiKyle/KishiKyle.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.physik.uni-muenchen.de/tk-svm/tksvm-op\n",
      "Could not parse repo_url: https://gitlab.physik.uni-muenchen.de/tk-svm/tksvm-op\n",
      "Could not parse repo_url: https://gitlab.physik.uni-muenchen.de/tk-svm/tksvm-op\n",
      "Could not parse repo_url: https://gitlab.physik.uni-muenchen.de/tk-svm/tksvm-op\n",
      "Could not parse repo_url: https://gitlab.com/maximelenormand/mismatch-connectivity\n",
      "Could not parse repo_url: https://bitbucket.org/realhubot/simservers\n",
      "Could not parse repo_url: https://gitlab.com/KaiyanP196/coevolution-disease-and-competing-opinions\n",
      "Could not parse repo_url: https://mu-lab.info/naoki_takashima/emer-cl\n",
      "Could not parse repo_url: https://bitbucket.org/pennyan/smtlink\n",
      "Could not parse repo_url: https://bitbucket.org/ktausch/psipy\n",
      "Could not parse repo_url: https://gitlab.com/anusser/frechet_distance_under_translation\n",
      "Could not parse repo_url: https://bitbucket.org/PanzerErik/kontsevInt\n",
      "Error processing repo https://github.com/Mengyanz/Mengyanz.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/cshwhale/M2HF: 404 {\"message\": \"This repository is empty.\", \"documentation_url\": \"https://docs.github.com/v3/repos/contents/#get-contents\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/spokoyny/spokoyny.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.com/simonknapen/dark_showers_tool\n",
      "Could not parse repo_url: https://huggingface.co/Idan0405/ClipMD\n",
      "Could not parse repo_url: https://gitlab.com/rotskoff/trajectory_estimators\n",
      "Could not parse repo_url: https://bitbucket.org/giorgiosarno/sl2cfoam-asym-1.0\n",
      "Could not parse repo_url: https://bitbucket.org/giorgiosarno/sl2cfoam-asym-1.0\n",
      "Could not parse repo_url: https://bitbucket.org/cgeoga/kernelmatrices.jl\n",
      "Could not parse repo_url: https://gitlab.com/tgpublic/tgindex\n",
      "Error processing repo https://github.com/saurabhsharma1993/mac: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.com/chen00217/bert_subsumption\n",
      "Could not parse repo_url: https://bitbucket.org/scimonucd/pybind11foam\n",
      "Could not parse repo_url: https://gitlab.com/smc/mlphon\n",
      "Could not parse repo_url: https://gitlab.com/mgadermayr/mixupmil\n",
      "Error processing repo https://github.com/akwasigroch/NAS_network_morphism: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.com/vcb-inclusive/npinb2xclv\n",
      "Could not parse repo_url: https://zenodo.org/record/10462251\n",
      "Error processing repo https://github.com/amrita-medical-ai/eusml-labeller: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/seannz/cvxq: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.com/formalv/formalv\n",
      "Could not parse repo_url: https://bitbucket.org/columbiadvmm/cdc\n",
      "Error processing repo https://github.com/rongyaofang/instructseq: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.com/auto_eft/autoeft\n",
      "Could not parse repo_url: https://gitlab.com/auto_eft/autoeft\n",
      "Could not parse repo_url: https://gitlab.com/leia-methods/lent\n",
      "Could not parse repo_url: https://gitlab.com/leia-methods/lent\n",
      "Could not parse repo_url: https://bitbucket.org/jjhw3/divergences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/gillesvntnu/gcn_multistructure to /repositories/696275958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 Python files in repository 'gcn_multistructure' for paper 'Towards Robust Cardiac Segmentation using Graph Convolutional Networks'.\n",
      "Error processing repo https://github.com/gillesvntnu/gcn_multistructure: 'list' object has no attribute 'setdefault'\n",
      "Error processing repo https://github.com/large-ocr-model/large-ocr-model.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/mzakariaCERN/mzakaria.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/iceplussss/QSAR-Complete: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://plmlab.math.cnrs.fr/stoch-algo-phys/generative-models/fixed-kinetic-NHF/\n",
      "Could not parse repo_url: https://gitlab.com/picture-production/picture-nnunet-package\n",
      "Error processing repo https://github.com/hbx-hbx/dynamics_of_zero-shot_generalization: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/Mengyanz/Mengyanz.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/Mengyanz/Mengyanz.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://huggingface.co/datasets/MLCommons/speech-wikimedia\n",
      "Could not parse repo_url: https://worksheets.codalab.org/worksheets/0x1ad3f387005c492ea913cf0f20c9bb89\n",
      "Error processing repo https://github.com/jp-sm/jp-sm.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Found 7 Python files in repository 'adf-20' for paper 'Adaptive Deep Forest for Online Learning from Drifting Data Streams'.\n",
      "Error processing repo https://github.com/adf-rep/adf-20: 'list' object has no attribute 'setdefault'\n",
      "Could not parse repo_url: https://gitlab.inria.fr/vmurukut/psl\n",
      "Error processing repo https://github.com/gacafe/gacafe.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://bitbucket.org/giorgiosarno/sl2cfoam-asym-1.0\n",
      "Could not parse repo_url: https://gitlab.com/n-ham-paper-files/generate-strategic-form-game\n",
      "Could not parse repo_url: https://gitlab.idiap.ch/bob/bob.paper.icassp2022_morph_generate\n",
      "Could not parse repo_url: https://gitlab.com/arun.prakash.mimm/optimic\n",
      "Could not parse repo_url: https://bitbucket.org/unizg-fer-lamor/kittical\n",
      "Could not parse repo_url: https://bitbucket.org/lanzithinking/geom-infmcmc\n",
      "Could not parse repo_url: https://gitlab.nccr-automation.ch/loris.dinatale/NoDRL\n",
      "Could not parse repo_url: https://bitbucket.org/unipv_cvmlab/sisfalltemporallyannotated\n",
      "Error processing repo https://github.com/hoangducthuong/hoangducthuong.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/hoangducthuong/hoangducthuong.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://git.openi.org.cn/OpenPointCloud/PCSOD\n",
      "Error processing repo https://github.com/Quarzon07/Qclicker-updated/blob/main/Qclicker%20(copy)/Q-clicker.py: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\", \"status\": \"404\"}\n",
      "Error processing repo https://github.com/ANDRIANTSIORY/MSC_parallel: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.com/ricky7guanyu/tensor-completion-with-regularization-term\n",
      "Error processing repo https://github.com/IntenF/IntenF.github.io: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\", \"status\": \"404\"}\n",
      "Could not parse repo_url: https://gitlab.com/nisec/ecckiila\n",
      "Could not parse repo_url: https://gitlab.com/greschai/shadowgrouping\n",
      "Could not parse repo_url: https://gitlab.com/flomertens/pspipe\n",
      "Could not parse repo_url: https://gitlab.com/aomediacodec/svt-av1\n",
      "Found 70 Python files in repository 'transfergraph' for paper 'Model Selection with Model Zoo via Graph Learning'.\n",
      "Found 15 Python files in repository 'leveraging-corpus-metadata' for paper 'Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition'.\n",
      "Found 0 Python files in repository 'fedia' for paper 'FedIA: Federated Medical Image Segmentation with Heterogeneous Annotation Completeness'.\n",
      "Found 2 Python files in repository 't4text' for paper 'To Tell The Truth: Language of Deception and Language Models'.\n",
      "Found 51 Python files in repository 'ai2apps' for paper 'AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /repos/vivek3141/ghostbuster-data/contents/reuter/gpt_semantic/EricAuchard failed with 403: Forbidden\n",
      "Setting next backoff to 255.19024s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 Python files in repository 'ghostbuster-data' for paper 'Ghostbuster: Detecting Text Ghostwritten by Large Language Models'.\n",
      "Found 4 Python files in repository 'react-emus' for paper 'Stage-IV Cosmic Shear with Modified Gravity and Model-independent Screening'.\n",
      "Found 23 Python files in repository 'vfl-admm' for paper 'Improving Privacy-Preserving Vertical Federated Learning by Efficient Communication with ADMM'.\n",
      "Found 125 Python files in repository 'OSFNTC' for paper 'An Open-Source Framework for Efficient Numerically-Tailored Computations'.\n",
      "Found 0 Python files in repository 'DiffSCI' for paper 'DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model'.\n",
      "Found 4 Python files in repository 'traffic_dynamics' for paper 'Spatiotemporal Implicit Neural Representation as a Generalized Traffic Data Learner'.\n",
      "Found 6 Python files in repository 'llm-ell' for paper 'A Law of Next-Token Prediction in Large Language Models'.\n",
      "Found 0 Python files in repository 'Truth_Discovery_Comparative_Analysis' for paper 'Truth Discovery Algorithms: An Experimental Evaluation'.\n",
      "Found 4906 Python files in repository 'lmops' for paper 'MiniLLM: Knowledge Distillation of Large Language Models'.\n",
      "Found 353 Python files in repository 'tempnet' for paper 'To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO'.\n",
      "Found 426 Python files in repository 'cotr' for paper 'COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction'.\n",
      "Found 8 Python files in repository 'rctnet' for paper 'Relating CNN-Transformer Fusion Network for Change Detection'.\n",
      "Found 0 Python files in repository 'emocommonsense' for paper 'VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning'.\n",
      "Could not parse repo_url: https://gitlab.com/multiloop-pku/blade\n",
      "Found 0 Python files in repository 'CPT' for paper 'Controllable Prompt Tuning For Balancing Group Distributional Robustness'.\n",
      "Found 135 Python files in repository 'dpo-trajectory-reasoning' for paper 'Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/roshnigiyer/br-gcn to /repositories/277449468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 Python files in repository 'br-gcn' for paper 'Hierarchical Attention Models for Multi-Relational Graphs'.\n",
      "Found 1 Python files in repository 'bayesoptimalalg' for paper 'Suboptimal Performance of the Bayes Optimal Algorithm in Frequentist Best Arm Identification'.\n",
      "Found 0 Python files in repository 'Rigged-Dynamic-Mode-Decomposition' for paper 'Rigged Dynamic Mode Decomposition: Data-Driven Generalized Eigenfunction Decompositions for Koopman Operators'.\n",
      "Found 57 Python files in repository 'RatchetEHR' for paper 'ICU Bloodstream Infection Prediction: A Transformer-Based Approach for EHR Analysis'.\n",
      "Found 14 Python files in repository 'misinformation-resilient-search-rankings' for paper 'Misinformation Resilient Search Rankings with Webgraph-based Interventions'.\n",
      "Found 43 Python files in repository 'memsave_torch' for paper 'Lowering PyTorch's Memory Consumption for Selective Differentiation'.\n",
      "Found 0 Python files in repository 'lahaja' for paper 'LAHAJA: A Robust Multi-accent Benchmark for Evaluating Hindi ASR Systems'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/typoverflow/OfflineRL-Lib to /repositories/600955627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 367 Python files in repository 'OfflineRL-Lib' for paper 'Extreme Q-Learning: MaxEnt RL without Entropy'.\n",
      "Found 15 Python files in repository 'gstran' for paper 'GSTran: Joint Geometric and Semantic Coherence for Point Cloud Segmentation'.\n",
      "Found 0 Python files in repository 'incompressiblenavierstokes.jl' for paper 'Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation'.\n",
      "Found 247 Python files in repository 'selfdenoise' for paper 'Advancing the Robustness of Large Language Models through Self-Denoised Smoothing'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/SAP-samples/security-research-membership-inference-and-differential-privacy to /repositories/314617670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 Python files in repository 'security-research-membership-inference-and-differential-privacy' for paper 'Membership Inference Attacks against Machine Learning Models'.\n",
      "Found 14 Python files in repository 'few-shot-fruit-segmentation-via-transfer-learning' for paper 'Few-Shot Fruit Segmentation via Transfer Learning'.\n",
      "Found 0 Python files in repository 'shopping-queries-image-dataset' for paper 'Shopping Queries Image Dataset (SQID): An Image-Enriched ESCI Dataset for Exploring Multimodal Learning in Product Search'.\n",
      "Found 353 Python files in repository 'mulan-code' for paper 'MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion'.\n",
      "Found 7 Python files in repository 'detecting-and-refining-hirise-image-patches-obscured-by-atmospheric-dust' for paper 'Detecting and Refining HiRISE Image Patches Obscured by Atmospheric Dust'.\n",
      "Found 120 Python files in repository 'reservoirpy' for paper 'Evolving Reservoirs for Meta Reinforcement Learning'.\n",
      "Found 13 Python files in repository 'promptreformulate' for paper 'Prompt Refinement with Image Pivot for Text-to-Image Generation'.\n",
      "Found 99 Python files in repository 'cc-shap-vlm' for paper 'Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?'.\n",
      "Found 0 Python files in repository 'bart-hitting-time-sims' for paper 'The Computational Curse of Big Data for Bayesian Additive Regression Trees: A Hitting Time Analysis'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/gnaiqing/llmdp to /repositories/673996450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 Python files in repository 'llmdp' for paper 'Can Large Language Models Design Accurate Label Functions?'.\n",
      "Found 155 Python files in repository 'gpt-researcher' for paper 'Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models'.\n",
      "Found 1 Python files in repository 'Unsupervised-Domain-Adaptation-by-Backpropagation-' for paper 'Unsupervised Domain Adaptation by Backpropagation'.\n",
      "Found 48 Python files in repository 'ilmart' for paper 'ILMART: Interpretable Ranking with Constrained LambdaMART'.\n",
      "Found 8 Python files in repository 'scarce' for paper 'Learning with Complementary Labels Revisited: The Selected-Completely-at-Random Setting Is More Practical'.\n",
      "Found 26 Python files in repository 'rl-tools' for paper 'RLtools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control'.\n",
      "Found 26 Python files in repository 'rl-tools' for paper 'Data-Driven System Identification of Quadrotors Subject to Motor Delays'.\n",
      "Found 7 Python files in repository 'BERT-WSD' for paper 'Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences'.\n",
      "Found 15 Python files in repository 'rgbs50' for paper 'RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer Tracker'.\n",
      "Found 0 Python files in repository 'multimodal' for paper 'Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning'.\n",
      "Found 1 Python files in repository 'hpclust' for paper 'High-Performance Hybrid Algorithm for Minimum Sum-of-Squares Clustering of Infinitely Tall Data'.\n",
      "Found 3 Python files in repository 'RTL-Repo' for paper 'RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects'.\n",
      "Found 45 Python files in repository 'PI-CLIP' for paper 'Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation'.\n",
      "Found 39 Python files in repository 'LoopSplat' for paper 'LoopSplat: Loop Closure by Registering 3D Gaussian Splats'.\n",
      "Found 324 Python files in repository 'map-neo' for paper 'MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/hansenhua/ilid-offline-imitation-learning to /repositories/802911252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 Python files in repository 'ilid-offline-imitation-learning' for paper 'How to Leverage Diverse Demonstrations in Offline Imitation Learning'.\n",
      "Found 22 Python files in repository 'adc' for paper 'Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning'.\n",
      "Found 49 Python files in repository 's3gaussian' for paper '$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving'.\n",
      "Found 13 Python files in repository 'Unveiling-Linguistic-Regions-in-LLMs' for paper 'Unveiling Linguistic Regions in Large Language Models'.\n",
      "Found 11 Python files in repository 'average-case-robustness' for paper 'Characterizing Data Point Vulnerability via Average-Case Robustness'.\n",
      "Found 98 Python files in repository 'mvst' for paper 'Multi-View Spectrogram Transformer for Respiratory Sound Classification'.\n",
      "Found 56 Python files in repository 'Text-Classification-MSFF-QDConv-Model' for paper 'Multi-Scale Feature Fusion Quantum Depthwise Convolutional Neural Networks for Text Classification'.\n",
      "Found 127 Python files in repository 'SwedishLLMBenchmark' for paper 'LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pickle\n",
    "import gc  # Import garbage collector\n",
    "\n",
    "def build_folder_tree(python_files):\n",
    "    tree = {}\n",
    "    for file_info in python_files:\n",
    "        rel_path = file_info.get(\"path\", file_info[\"name\"])\n",
    "        parts = rel_path.split('/')\n",
    "        current_level = tree\n",
    "        for part in parts[:-1]:\n",
    "            current_level = current_level.setdefault(part, {})\n",
    "        current_level.setdefault(\"files\", []).append(parts[-1])\n",
    "    return tree\n",
    "\n",
    "base_download_dir = \"/media/ll16598/One Touch/INNOVATE/pwc_python_files_from_git/\"\n",
    "\n",
    "paper_python_files = {}\n",
    "paper_folder_structures = {}\n",
    "\n",
    "for e, entry in enumerate(paper_code):\n",
    "    repo_url = entry.get('repo_url')\n",
    "    paper_title = entry.get('paper_title', 'unknown_paper')\n",
    "    if sanitize_filename(paper_title).replace(\" \", \"_\") in completed_titles:\n",
    "        continue\n",
    "    completed_titles.append(paper_title)\n",
    "    if not repo_url:\n",
    "        print(f\"No repo_url found for paper: {paper_title}\")\n",
    "        continue\n",
    "    \n",
    "    owner, repo_name = get_repo_from_url(repo_url)\n",
    "    if not owner or not repo_name:\n",
    "        print(f\"Could not parse repo_url: {repo_url}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        repo = g.get_repo(f\"{owner}/{repo_name}\")\n",
    "        python_files = get_all_python_files(repo)\n",
    "        paper_python_files[paper_title] = python_files\n",
    "        print(f\"Found {len(python_files)} Python files in repository '{repo_name}' for paper '{paper_title}'.\")\n",
    "\n",
    "        folder_structure = build_folder_tree(python_files)\n",
    "        paper_folder_structures[paper_title] = folder_structure\n",
    "\n",
    "        safe_title = sanitize_filename(paper_title).replace(\" \", \"_\")\n",
    "        paper_dir = os.path.join(base_download_dir, safe_title)\n",
    "        os.makedirs(paper_dir, exist_ok=True)\n",
    "    \n",
    "        for file_info in python_files:\n",
    "            file_url = file_info[\"download_url\"]\n",
    "            rel_path = file_info.get(\"path\", file_info[\"name\"])\n",
    "            file_path = os.path.join(paper_dir, rel_path)\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(file_url)\n",
    "                response.raise_for_status()\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(response.text)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                # Important: close the response to release its connection/ buffers\n",
    "                response.close()\n",
    "                # Delete the response object so it can be garbage-collected\n",
    "                del response\n",
    "\n",
    "        # Pickle updates (still optional, but presumably you want to do it here)\n",
    "        with open(os.path.join(base_download_dir, f'paper_python_links_{e}.pickle'), 'wb') as f:\n",
    "            pickle.dump(paper_python_files, f)\n",
    "        with open(os.path.join(base_download_dir, f'paper_folder_structures_{e}.pickle'), 'wb') as f:\n",
    "            pickle.dump(paper_folder_structures, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing repo {repo_url}: {e}\")\n",
    "    \n",
    "    # Force a collection at the end of each main loop iteration\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
